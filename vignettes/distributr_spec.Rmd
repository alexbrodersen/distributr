---
title: "distributr prototype"
author: "Patrick Miller"
date: "August 3 2016"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
library(xtable)
library(diagram)
library(distributr)
library(dplyr)

knitr::opts_chunk$set(echo = TRUE, eval=F)
```

The motivation for this package is to provide a flexible but fun interface for distributed computing on large computing clusters. It's my impression that cluster resources are usually shared, and jobs are administered using a job manager like Sun Grid Engine or Torque. This produces an 'asynchronous' work flow were users specify jobs/resources, submit jobs, and collect results. These jobs can often take days or weeks to complete. I've found it to be unpleasant to learn the idiosyncrasies of these systems. I've also found myself copying 'glue' code I use from one project to another. The purpose of the package is to simplify usage of distributed computing in a familiar way.

Some common applications for distributed computing in statistics are

- Monte Carlo simulations 
- meta-parameter tuning for machine learning
- distributed data analysis
- computationally intensive workflows and pipelines.

The motivation for the package is to exploit the fact that the tasks are embarrassingly parallel and modular. Data is generated or loaded, then one or more analyses are carried out on the data, and results are computed and aggregated. Often the analysis or data generating functions are applied to a 'grid' of their arguments/parameters (e.g. tuning machine learning algorithms, or generating data under different conditions). This process is often carried out repeatedly (e.g. cv-folds, bootstrap replications, or Monte-Carlo replications). 

The main problem the package solves is providing a convenient interface for generating the 'glue' code that's necessary to carry out these tasks on a distributed computing cluster. A good abstraction could cover 80-90\% of the common tasks in these applications, and make using large clusters less painful to use and more fun. Some of the features of such a framework would be:

- easily modifying the workflow, adding conditions/replications without recomputation
- tidy results 
- memorable interface
- catching errors and warnings
- balancing memory use and run time
- works on multiple schedulers (SGE, Torque), or specific back ends like Amazon EC2 or Hadoop

Below, I describe a simple function that covers a basic workflow, and a modular solution that is more flexible. The single function (`gapply`) applies a function to a grid of its arguments. All the steps in a workflow are encapsulated in a single function, which is applied to the argument grid. This is simple to use, but it is not easy to add steps or modify workflows.

We can get a more modular solution by noting that a workflow is a graph of functions. This graph can be described by nodes and layers constructed using `node` and `layer` respectively. Sequential steps in a workflow can be described by adding layers using `+`. This enables easy modification of workflows by adding nodes to existing layers, or adding new layers. It is similar in spirit to ggplot2. 

# gapply

This function (_grid_ apply) covers the basic use case of repeatedly applying a function over a grid of its arguments in parallel. The function written by the user performs all steps in a workflow. For example, generating / loading data, performing analyses, and returning results. An implementation of `gapply` is available on github.

```{r, eval=TRUE}
#devtools::install_github("patr1ckm/distributr")
library(distributr) 
do.one <- function(n, mu, sd){ mean(rnorm(n, mu, sd)) }

sim <- gapply(do.one, n = c(50, 100, 500), mu = c(1,5), sd = c(1, 5, 10), 
              .reps=50, .mc.cores=5)
head(sim)
```


Using multiple performance criteria and multiple methods can be handled by returning a `data.frame` in `do.one`. The row and column names are mapped to `key` and `key2` in the results.

```{r, eval=TRUE}
do.one <- function(a=1,b=2){
  if(a==1){ stop("asdf")}
  if(b==2){warning("this is a warning")}
  return(data.frame(ops1=c(a+b, a-b), ops2=c(a*b, a^2)))
}

sim <- gapply(do.one, a=c(2,1), b=2, .reps=2, .verbose=0)
sim
```

Warnings/errors can be extracted as follows.

```{r, eval=TRUE}
err(sim)
warn(sim)
```
### Execution on Sun Grid Engine

I've implemented and used this code for on a computing cluster that uses Sun Grid Engine. I run this code on the head node.

- `setup` distributes conditions over nodes, and reps are computed within each node. The `.mc.cores` allows the replications to be computed in parallel within each node. `.chunks` allows splitting the replications over nodes.
- `submit` submits all jobs to the scheduler
- `collect` collects completed results 

```{r}
sim <- gapply(do.one, n = c(50, 100, 500), mu = c(1,5), sd = c(1, 5, 10), .eval=F)
setup(sim, .reps=500, .chunks = 3, .mc.cores = 5)
submit(sim)   
res <- collect(sim)
```


# Workflows with nodes and layers

Though using `gapply` with a single function `do.one` is convenient, it is not modular enough to add or edit steps in the workflow without recomputation. Additionally, with complex workflows, `do.one` can become tedious to maintain and debug.

A workflow can be viewed as a graph of functions. Each node is a function. The provided function `node` allows user defined functions to be applied to grids of parameters. Groups of nodes are organized as layers. The function `layer` simply takes multiple nodes. For example, a layer might contain multiple data generating functions or analysis methods. Finally, steps in a workflow are expressed by using `+` to add layers together. In this way, the output from nodes in a previous layer becomes the input to nodes in the current layer.

Consider the following example specification. A user defined function `f` generates or loads data. It is applied to a grid of `arg1, arg2`. Next, two user functions `g` and `g2` fit a model to that data, with their own arguments `arg3, arg4, arg5`. Finally, the function `h` aggregates results from both `g` and `g2`. Finally, simulation options can be specified using `options`, and
the various backend functions are provided (here, `sge` for Sun Grid Engine).


```{r, eval=F}
workflow <- layer(node(f, arg1=c(1,2), arg2=c(T, F))) +
  layer(node(g, arg3=c(.05, .001), arg4=c(10, 100)) +
        node(g2, arg5=c(1,2)) +   
  layer(node(h), node(h2)) +
  options(reps = 1000, tidy = TRUE, seed = 5) +  # workflow options
  sge(queue = "long", dir = "test", shell = "bash", ...)  # options to sge environment
```

- `node` takes a user supplied function, allows it to be applied to a grid of its argumentss, and specifies the output on which it is applied.
- `+` Generates the function graph by adding layers. In addition, workflow options and specification of the backend environment can also be added.
- `reps` specifies the number of replications
- `tidy` tidies output at the last level, otherwise list is returned
- `seed` sets seed at given levels using L'Ecuyer-CMRG RNG for reproducibility
- `save` specifies at what levels the output is saved
- `sge` is the backend on which the workflow is run, other backends are possible

Both layers and nodes have default ids or can have their ids set by the user. These ids are used to modify workflows and specify dependencies. For example, some nodes should not applied to all nodes in the previous layer. The argument `.dep` of the function `node` can be used to specify that the node should only be applied to a specific node given by it's id. 

The above workflow or simulation can be discribed in the following graph:

```{r, echo=F, eval=T, fig.width=5, fig.height=4}
 par(mar = c(1, 1, 1, 1), mfrow = c(1, 1), oma=c(1,1,1,1))
 names <- c("f()", "g()", "g2()", "h()")
M <- matrix(nrow = 4, ncol = 4, byrow = TRUE, data = 0)
M[2, 1] <- M[3, 1] <- M[4, 2] <- M[4, 3] <- ""
plotmat(M, pos = c(1, 2, 1), name = names, lwd= 1, relsize=1,
         box.lwd = 1, cex.txt=.8, box.type = "circle", box.prop=1.0)

#par(mar = c(1, 1, 1, 2), oma=c(1,1,1,3))
#names <- c("f()", "g()", "g2()", "h()", "h2()", "h()", "h2()")
#M <- matrix(nrow = 7, ncol = 7, byrow = TRUE, data = 0)
#M[2, 1] <- M[3, 1] <- M[4, 2] <- M[6, 3] <- M[5, 2] <- M[7, 3] <- ""
#plotmat(M, pos = c(1, 2, 4), name = names, lwd= 1, relsize=1,
#        box.lwd = 1, cex.txt=.8, box.type = "circle", box.prop=1.0)
xpos <- .95
text(xpos,.85, labels = "Level 1", las=.5, xpd=NA)
lines(x = c(xpos, xpos), y = c(.8, .55), xpd=NA)
text(xpos,.5, labels = "Level 2", las=.5, xpd=NA)
lines(x = c(xpos, xpos), y = c(.45, .2), xpd=NA)
text(xpos,.15, labels = "Level 3", las=.5, xpd=NA)

 
```

### Operations on `workflow`

It would be convenient to summarize the workflow in terms of the argument grid and the function  graph. 

- `plot` plots the function graph (as above)
- `summary` summarizes the grid of arguments
- `test` tests one or a few conditions, and estimates memory use and execution time
- `submit` submits jobs to the scheduler
- `collect` collects the results.

```{r, eval=F}
summary(workflow)
plot(workflow)
test(workflow)     
submit(workflow)
res <- collect(workflow)	

```

### Operations on results

- `summary` aggregates the `value` column of results using a function `.fun` (default `mean`) if results are tidy
- `errors`, `warnings` list errors or warnings

```{r, eval=F}
summary(res, .fun = mean, .reps = NULL)  		
err(res)       
warn(res)  		
```

### Testing and debugging

Workflows can be debugged interactively by simply computing parts of the function graph by hand in base R, e.g. `f() %>% g() %>% h()`. 

### Modifying workflows

New nodes or layers can be added to the workflow using layer ids. Below I show examples of adding new nodes at all levels. The figure shows what gets recomputed.


```{r, eval=F}
workflow <- workflow + layer(grid(f2, arg6 = c(5:8)), .id = 1)   

submit(workflow)
res <- collect(workflow)

workflow <- workflow + 
  layer(grid(g3, ..., .id = 2)) +
 layer(h3(.dep=c("g3"), .id = 3)

submit(workflow)
res <- collect(workflow)
```

```{r, echo=F, eval=T, fig.width=8, fig.height=4}
 par(mar = c(1, 1, 1, 1), mfrow = c(1, 2), oma=c(1,1,1,1))
 names <- c("f()", "f2()", "g()", "g2()", "h()")
M <- matrix(nrow = 5, ncol = 5, byrow = TRUE, data = 0)
M[3, 1:2] <- M[4, 1:2] <-  ""
M[5, 3:4] <- ""

update <- matrix(nrow = 5, ncol = 5, byrow = TRUE, data = "gray")
update[3:4, 2] <- "black"
update[5,3:4] <- "black"

plotmat(M, pos = c(2, 2, 1), name = names, lwd= 1, relsize=1,
         box.lwd = 1, cex.txt=.8, box.type = "circle", box.prop=1.0,
         arr.lcol=update, arr.col=update, box.lcol=c("gray", "black", "black", "black", "black"))

xpos <- .975
text(xpos,.85, labels = "Level 1", las=.5, xpd=NA)
lines(x = c(xpos, xpos), y = c(.8, .55))
text(xpos,.5, labels = "Level 2", las=.5, xpd=NA)
lines(x = c(xpos, xpos), y = c(.45, .2))
text(xpos,.15, labels = "Level 3", las=.5, xpd=NA)

par(mar = c(0, 0, 0, 0))
names <- c("f()", "f2()", "g()", "g2()", "g3()", "h()", "h3()")
M <- matrix(nrow = 7, ncol = 5, byrow = TRUE, data = 0)
M[3, 1:2] <- M[4, 1:2] <-  M[5, 1:2] <- ""
M[6, 3:4] <- M[7, 5] <- ""
update <- matrix(nrow = 7, ncol = 5, byrow = TRUE, data = "gray")
update[5, 1:2] <- update[7, 5] <- "black"

plotmat(M, pos = c(2, 3, 2), name = names, lwd= 1, relsize=1,
         box.lwd = 1, cex.txt=.8, box.type = "circle", box.prop=1.0,
        arr.lcol=update, arr.col=update, 
        box.lcol=c(rep("gray", 4), "black", "gray", "black"))
```

## Example: GBM meta-parameter tuning by cross-validation

```{r,eval=T, cache=T, messages=F}

library(gbm)
data("mpg",package="ggplot2") 
is.fac <- sapply(mpg, is.character)
mpg[,is.fac] <- lapply(mpg[,is.fac], as.factor)

do.one <- function(fold, ...){
  set.seed(104)
  fold.ids <- sample(1:5, size =  nrow(mpg), replace=T)
  test <- mpg[fold.ids == fold, ]
  out <- gbm(cty ~ . - hwy, data = mpg[fold.ids != fold, ], distribution="gaussian", ...)
  nt <- suppressWarnings(gbm.perf(out, method="OOB", plot.it=F))
  yhat <- predict(out, newdata = test, n.trees=nt)
  mse <- var(test$cty - yhat)
  return(c(mse=mse))
}

do.one(fold=1, n.trees=5) # test

cv.tune <- gapply(do.one, fold=1:5, n.trees=c(1000, 5000), 
       shrinkage=c(.01, .001, .005), interaction.depth=c(1:3), .mc.cores=5)

library(dplyr)
cv.tune %>% group_by(n.trees, shrinkage, interaction.depth) %>% 
  summarise(mse=mean(value)) %>%
  ungroup %>%
  arrange(mse) %>% head(n=5)


```

## Previous work

There has been a lot of good work already done in this vein, which has influenced and motivated the current design. 

`BatchJobs` and `BatchExperiments` support a fully asynchronous workflow, and support various backends including SGE, Torque, and SSH. The interface is maximally flexible with many of the features described above. But it was difficult to use and remember the function names and argument structure. I also found it difficult to reason about errors and bugs when they occurred.

Other work in R includes

- `simsalapar` Not asynchronous, not extendable/modular enough for many tasks, but good for simple cases.
- `ezsim` Not asynchronous, designed around the estimator/parameter metaphor. Specific to Monte Carlo simulation in statistics.
- `harvestr` Not asynchronous, weird but sort of happy farming metaphor for functions.
- `simFrame` Not asynchronous, specific data generation and analysis functions for simulations involving survey sampling and missing data.
- `simSummary` Only simplifies results aggregation.
- `mlr`, `caret` specifically designed for tuning machine learning models
- I contributed to `simsem`, a framework for Monte Carlo simulations with structural equation models. It also is not asynchronous, and focuses only on this class of models.
- Many other examples for simulating data under specific models.

There are many other 'distributed computing workflow' tools for other languages, e.g. `make`, `MakeFlow`, etc. However, it would be convenient to have an idiomatic R way to express these tasks.



